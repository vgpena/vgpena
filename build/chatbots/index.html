<!doctype html> <html lang=en-us> <head> <meta charset=utf-8> <link rel=alternate type="application/atom+xml" href="https://vgpena.github.io/feed.xml" title="Like, subscribe, &amp; download my mixtape"/> <meta property="og:type" content=website> <meta property="og:url" content="https://vgpena.github.io/chatbots/"> <meta property="og:site_name" content="hey it's violet"> <meta property="og:title" content="Turning Subreddits Into Chatbots"> <meta property="og:description" content="We were given two days and the theme of 'echo chamber'. We used subreddits to create chatbots and had them converse."> <meta property="og:image" content="https://vgpena.github.io/images/ahd/show2.jpg"> <meta name="twitter:domain" value="vgpena.github.io"> <meta property="twitter:title" content="Turning Subreddits Into Chatbots"> <meta property="twitter:description" ontent="We were given two days and the theme of 'echo chamber'. We used subreddits to create chatbots and had them converse."> <meta property="twitter:url" content="https://vgpena.github.io/chatbots/"> <meta property="twitter:image" content="https://vgpena.github.io/images/ahd/show2.jpg"> <meta name=author content="Violet PeÃ±a"> <meta name=description content="We were given two days and the theme of 'echo chamber'. We used subreddits to create chatbots and had them converse."> <meta name=viewport content="width=device-width, initial-scale=1"> <script>
  if ('FontFace' in window) {
    const fonts = [
      {
        name: 'Overpass Mono',
        files: [
          {
            fileName: 'OverpassMono-Light',
            weight: 'normal',
            style: 'normal',
          },
          {
            fileName: 'OverpassMono-Bold',
            weight: 'bold',
            style: 'normal',
          }
        ]
      },
      {
        name: 'Pitch Display',
        files: [
          {
            fileName: 'Pitch-Display',
            weight: 'normal',
            style: 'normal',
          }
        ]
      },
      {
        name: 'Karla',
        files: [
          {
            fileName: 'Karla-Regular',
            weight: 'normal',
            style: 'normal',
          },
          {
            fileName: 'Karla-Bold',
            weight: 'bold',
            style: 'normal',
          },
          {
            fileName: 'Karla-Italic',
            weight: 'normal',
            style: 'italic',
          },
          {
            fileName: 'Karla-BoldItalic',
            weight: 'bold',
            style: 'italic',
          },
        ]
      },
    ];

    fonts.forEach((font) => {
      font.files.forEach((file) => {
        const currFont = new FontFace(
          font.name,
          `url(/fonts/${ file.fileName }.woff2), url(/fonts/${ file.fileName }.woff)`,
          {
            weight: file.weight,
            style: file.style,
          }
        );

        currFont.load().then(() => {
          document.fonts.add(currFont);
        });
      });
    });
  }
</script> <script>
      window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
      ga('create', 'UA-92096829-1', 'auto');
      ga('send', 'pageview');
    </script> <script async src='https://www.google-analytics.com/analytics.js'></script> <title>Turning Subreddits Into Chatbots | hey it's violet</title> <link href="/styles/styles.css" rel=stylesheet media=screen /> </head> <body> <header id=site-header role=banner aria-label="Hey it's Violet: a blog"> <h1 id=site-title> <a href="/" aria-label="Back to home">Hey it's Violet</a> </h1> </header> <main> <article aria-labelledby=article-title> <heading id=article-heading role=heading> <h1 id=article-title> Turning Subreddits Into Chatbots </h1> <div id=article-info> <h2 class=article-info-piece id=article-subtitle> See also: BigQuery, Unity, and Markov chains </h2> <h2 class=article-info-piece id=article-date aria-label="Published April 02, 2017"> 4.2.2017 </h2> </div> </heading> <div class=content> <p>This past weekend, I attended <a href="http://www.arthackday.net/events/echo-chamber">Art Hack Day: Echo Chamber</a>. On Friday evening, the participants introduced ourselves and shared ideas; by Sunday evening, I had, as part of a group, created and exhibited Reddit-powered chatbots. It was an amazing, exhilarating weekend and I came out of it tired but more inspired than I thought possible.</p> <h2 class=section-title id=art-hack-the-planet><a href='#art-hack-the-planet' class=section-inner>(Art) Hack the Planet</a></h2> <p><figure role=img class="image-wrap image-secondary"> <img src="/images/ahd/pnca.jpg" alt="A photo of a blocky, light grey building. The letters &quot;PNCA&quot; run up its side." title="The PNCA building in downtown Portland" width=500px height=667px /> <figcaption> <p>PNCA -- my home for the weekend. <a href="http://gabesimagination.com/">credit</a></p> </figcaption> </figure></p> <p><a href="http://www.arthackday.net/">Art Hack Day</a> (AHD) is a series of events that brings together tecchies and artists. Participants divide into ad hoc groups, work on art projects for a day and a half, and then the weekend culminates in an art show. This time around, we were in the uber-beautiful <a href="http://pnca.edu/">PNCA</a> and sponsored in part by their interdisciplinary <a href="http://pnca.edu/makethinkcode">Make+Think+Code</a>.</p> <p>Part of what&#39;s lovely about AHD is that, as the name implies, they go out of their way to create a balance between art and tech. This isn&#39;t a hack day to make an app or redesign a website; it&#39;s a time to collaborate with people of different skill sets, push your boundaries, and create something interesting but not necessarily useful.</p> <h2 class=section-title id=do-you-want-to-build-a-chatbot><a href='#do-you-want-to-build-a-chatbot' class=section-inner>Do You Want to Build a Chatbot?</a></h2> <p>The theme of this AHD was &quot;Echo Chamber&quot;, particularly with an eye towards the internet and social media, which easily become echo chambers that do little more than affirm our already-held beliefs. This creative prompt got me thinking about chatbots, which I&#39;ve been into since I was introduced to <a href="https://en.wikipedia.org/wiki/SmarterChild">SmarterChild</a> in middle school.</p> <p><figure role=img class="image-wrap image-secondary"> <img src="/images/ahd/smarterchild.jpg" alt="A screengrab of an old AIM conversation with SmarterChild." title="A screengrab of a typically awful conversation with SmarterChild" width=242px height=386px /> <figcaption> <p>A typical interaction with SmarterChild. <a href="http://yaleherald.com/bullblog/aim-bots-where-are-they-now/">source</a></p> </figcaption> </figure></p> <p>On Friday evening, as we went around the room and introduced ourselves, a couple others indicated an interest in chatbots -- <a href="https://twitter.com/ginkko">Alec Arme</a> and <a href="http://www.poinyent.com/">Steve August</a>. We got together and started brainstorming ideas, which ranged from assigning points to bots&#39; arguments and picking a winner, to creating bots that represented different Greek gods. When we left for the night, we knew that:</p> <ul> <li>we wanted to make several bots, not just one;</li> <li>those bots would be trained or given personalities based on different data sources; and</li> <li>we wanted those bots to somehow talk to each other.</li> </ul> <p>At this point, I should mention that none of us had experience with chatbots, and that none of us had worked together before. We have very different skill sets -- Alec works mostly with VR, Steve specializes in psychology and using tech to create calm, and I do web development and browser-based installations. Whatever our end product would be, we knew it wouldn&#39;t be typical.</p> <h2 class=section-title id=enter-bigquery><a href='#enter-bigquery' class=section-inner>Enter BigQuery</a></h2> <p>Any bot needs source material, and we had decided that our bots would be trained on internet communities that could be construed as echo chambers. Even before we were sure what the exact training material would be, we knew that we would have to process a <strong>lot</strong> of data in order to accurately reflect those communities.</p> <p>When I think &quot;big data&quot;, I think of <a href="https://cloud.google.com/bigquery/">Google BigQuery</a>. I hadn&#39;t worked with it before, but I <strong>had</strong> had the pleasure of working alongside a team creating <a href="http://www.instrument.com/work/google-next-cloud-platform">a BigQuery-powered trivia game</a>, so I was already familiar with BigQuery as a service and knew the basics of how to use it.</p> <p>Essentially, BigQuery is a pay-as-you-go service that lets you run SQL queries over humongous data sets in disgustingly little time. It opens the door to people like me, who want to crunch dozens of gigabytes of data, but don&#39;t have access to powerful machines.</p> <p>As if this weren&#39;t enough, BigQuery hosts <a href="https://www.reddit.com/r/bigquery/wiki/datasets">tons of publically available datasets</a>, so we didn&#39;t have to manually write scrapes of comment sections or format those results to be BigQuery-compatible. (My favorite dataset is this 452GB one of <a href="https://github.com/odota/core/issues/924">DotA gameplay data</a>).</p> <hr> <p>The dataset that spoke to us the most was the one of <a href="https://bigquery.cloud.google.com/dataset/fh-bigquery:reddit_comments">all Reddit comments since 2005</a>. <a href="https://www.reddit.com/">Reddit</a>, &quot;the front page of the internet&quot;, is divided into subreddits dedicated to specific hobbies or interests. As such, they can easily become echo chambers where all participants agree with each other and validate each others&#39; beliefs.</p> <p>We decided to base our bots off of specific subreddits&#39; comments from December 2016 to February 2017 -- a cool 52.9GB of data in all. But how would we turn this data source into usable material for chatbots?</p> <h2 class=section-title id=creating-a-training-corpus><a href='#creating-a-training-corpus' class=section-inner>Creating a Training Corpus</a></h2> <p>When I want to generate language, I immediately think of <a href="https://en.wikipedia.org/wiki/Markov_chain">Markov chains</a>. Also known as Markov models, these are processes that let us use statistics collected about an input to probabilistically generate an output.</p> <p>You can create a Markov chain by going through a body of text (&quot;training corpus&quot;) and counting how many times each word occurs, and for each word, what words follow it and how often. You create a profile of the text: how often can you expect a particular word to come up, and what other words you expect to follow it? You end up with a formula for creating utterances &quot;in the style&quot; of your training corpus.</p> <p>To use subreddits as a training corpus, we ran the following SQL query in BigQuery against the aforementioned Reddit comment tables:</p> <div class=highlight><pre class="code sql"><code><span class="k">SELECT</span> <span class="n">word</span><span class="p">,</span> <span class="n">nextword</span><span class="p">,</span> <span class="k">COUNT</span><span class="p">(</span><span class="o">*</span><span class="p">)</span> <span class="k">c</span>
<span class="k">FROM</span> <span class="p">(</span>
  <span class="k">SELECT</span>
    <span class="n">pos</span><span class="p">,</span>
    <span class="n">word</span><span class="p">,</span>
    <span class="n">LEAD</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="n">OVER</span><span class="p">(</span><span class="k">ORDER</span> <span class="k">BY</span> <span class="n">pos</span><span class="p">)</span> <span class="n">nextword</span>
  <span class="k">FROM</span> <span class="p">(</span>
    <span class="k">SELECT</span>
      <span class="n">word</span><span class="p">,</span>
      <span class="n">pos</span>
    <span class="k">FROM</span>
      <span class="n">FLATTEN</span><span class="p">(</span> <span class="p">(</span>
        <span class="k">SELECT</span>
          <span class="n">word</span><span class="p">,</span>
          <span class="k">POSITION</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="n">pos</span>
        <span class="k">FROM</span> <span class="p">(</span>
          <span class="k">SELECT</span>
            <span class="n">SPLIT</span><span class="p">(</span><span class="n">REGEXP_REPLACE</span><span class="p">(</span><span class="k">LOWER</span><span class="p">(</span><span class="n">body</span><span class="p">),</span> <span class="n">r</span><span class="s1">'(</span><span class="se">\n</span><span class="s1">)'</span><span class="p">,</span> <span class="n">r</span><span class="s1">' '</span><span class="p">),</span> <span class="s1">' '</span><span class="p">)</span> <span class="n">word</span>
          <span class="k">FROM</span>
            <span class="p">[</span><span class="n">fh</span><span class="o">-</span><span class="n">bigquery</span><span class="p">:</span><span class="n">reddit_comments</span><span class="p">.</span><span class="mi">2017</span><span class="n">_02</span><span class="p">],</span> <span class="p">[</span><span class="n">fh</span><span class="o">-</span><span class="n">bigquery</span><span class="p">:</span><span class="n">reddit_comments</span><span class="p">.</span><span class="mi">2017</span><span class="n">_01</span><span class="p">],</span> <span class="p">[</span><span class="n">fh</span><span class="o">-</span><span class="n">bigquery</span><span class="p">:</span><span class="n">reddit_comments</span><span class="p">.</span><span class="mi">2016</span><span class="n">_12</span><span class="p">]</span> <span class="k">WHERE</span> <span class="n">subreddit</span> <span class="o">=</span> <span class="s1">'dankmemes'</span> <span class="k">LIMIT</span> <span class="mi">10000</span>
         <span class="p">)</span> <span class="p">),</span> <span class="n">word</span><span class="p">)</span> <span class="p">))</span>
         <span class="k">WHERE</span> <span class="k">length</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">1</span> <span class="k">and</span> <span class="k">length</span><span class="p">(</span><span class="n">nextword</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">1</span> <span class="k">and</span> <span class="n">word</span> <span class="o">!=</span> <span class="n">nextword</span>
<span class="k">GROUP</span> <span class="k">BY</span>
  <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span>
  <span class="k">ORDER</span> <span class="k">BY</span>
  <span class="k">c</span> <span class="k">DESC</span>
</code></pre></div> <p>This is a modified version of the code provided by <a href="https://twitter.com/felipehoffa">Felipe Hoffa</a> on <a href="http://blog.gdeltproject.org/making-ngrams-bigquery-scale/">the GDELT blog</a>. It reads in the specified tables (<code>[fh-bigquery:reddit_comments.2017_02]</code>, etc.) and creates a new table of words, &#39;nextword&#39;s, and the count (&#39;c&#39;) of that combination.</p> <p>Training corpora were created by swapping out the subreddit we looked for -- <code>WHERE subreddit = &#39;dankmemes&#39;</code> makes our query only count words in comments that had <code>dankmemes</code> listed as a subreddit. We cleaned up the data a bit to make processing easier -- words were all lowercased so we wouldn&#39;t have to deal with capitalization, and we stripped out line breaks.</p> <p>We created nine training corpora this way, based on output tables ranging in size from 513KB to 7.16MB. Tolstoy&#39;s 864-page <em>Anna Karenina</em>, for comparison, takes up about 1.9MB. We had plenty of data; we were ready to make some bots.</p> <h2 class=section-title id=back-to-the-frontend><a href='#back-to-the-frontend' class=section-inner>Back to the Frontend</a></h2> <p>While I was getting the data and reworking it into a usable form, Alec and Steve were putting together our project&#39;s frontend. We had decided that we wanted to make something that looked like a chat app, and have our bots appear to text each other as they conversed.</p> <p>Since the bulk of my work has been web-based, even my installations have leaned heavily on Google Chrome as a frontend. Alec, however, is most experienced developing with <a href="https://unity3d.com/">Unity</a>, a game engine used widely for 3D games and Virtual Reality experiences. I associate Unity so tightly with those types of projects that I was astounded when Alec volunteered to make our 2D frontend in Unity. It turns out that Unity can also be a powerful way to make UIs and other 2D products, and that Alec knew a Unity plugin that would let us create a connection between his Unity frontend and my <a href="https://nodejs.org/">Node.js</a> backend with WebSockets.</p> <p>Alec and Steve teamed up on creating a frontend that mimicked an Android texting UI, complete with user avatars pulled from the subreddits&#39; header images. We would randomly choose two bots and have them &quot;converse&quot; with each other. The conversation would go on for around twenty utterances per bot; then we would choose two new bots and start again.</p> <p><figure role=img class="image-wrap image-secondary"> <img src="/images/ahd/test1.jpg" alt="A photo of a laptop screen showing our project. A mocked Android texting UI shows language generated by our bots." title="One of our first tests." width=800px height=1067px /> <figcaption> <p>It&#39;s alive! One of our first tests.</p> </figcaption> </figure></p> <p>The frontend sent, via WebSockets, an event to the backend requesting an utterance from a specific bot. The backend would generate an utterance on the spot and send that text back. The frontend controlled the conversation flow; it would wait to request a new utterance based on how long the previous utterance would take to read.</p> <p>We set up a router to let our laptops talk to each other via local IPs (the building&#39;s WiFi wasn&#39;t cooperating), and voil&agrave; -- we were able to run our first tests. The actual installation would have the server and frontend on the same computer; we wouldn&#39;t even need an internet connection.</p> <h2 class=section-title id=making-bots-chat><a href='#making-bots-chat' class=section-inner>Making Bots Chat</a></h2> <p>The last stage of the project was cleanup -- on the frontend, finessing assets and behavior; on the backend, stripping out badly-encoded characters (which appeared as &#65533;) and trying to get the bots to talk to each other.</p> <p>So far we had been generating utterances apropos of nothing, probabilistically picking a starting word from a bot&#39;s corpus and building the rest of the utterance from there. Our goal, though, was to make the bots&#39; texting feel like a conversation, which meant relating their utterances somehow. One way to do this is to relate utterances to predetermined &quot;topics&quot; and pick appropriate responses filed under that same topic. Steve had looked into achieving this with <a href="http://www.alicebot.org/aiml.html">AIML</a>, a markup language for chatbots, earlier on in the weekend. That approach, however, required a smaller, more manually-determined set of utterances than what we had on hand.</p> <p>We decided it would be enough to get the bots to use the same word across utterances, even if we couldn&#39;t guarantee a match between their utterances&#39; actual themes.</p> <p>We started tracking every utterance in the conversation, ranking the words in an utterance from least- to most-used, assuming that the most common words (&quot;a&quot;, &quot;the&quot;, &quot;I&quot;) are the least important to establishing a theme and that more unique words (&quot;Iraq&quot;, &quot;congress&quot;, &quot;emails&quot;) are more indicative of an utterance&#39;s theme. When generating a new utterance, we would pick its starting word by trying to find one of the more &quot;interesting&quot; words from the previous utterance in the current bot&#39;s corpus. Some words were only known by certain bots, but more often than not, the desired word was known by both bots, albeit with different connotations.</p> <p>Here&#39;s a sample exchange between two bots that demonstrates &quot;staying on topic&quot;:</p> <blockquote> <p><strong>dankmemes</strong>: rights die.</p> <p><strong>The_Donald</strong>: die.</p> <p><strong>dankmemes</strong>: what yeah, dank [deleted] dank great?</p> <p><strong>The_Donald</strong>: yeah, oh the third was woman drones. is officials.</p> <p><strong>dankmemes</strong>: yeah, you bot, eating questions re-submit harder being ^^/r/ayylmao2dongerbot thrusting regret a the in actually methinks. basking that get brethren, this more to words.</p> <p><strong>The_Donald</strong>: yeah, oh so.. don&#39;t hate what that wtf!!!</p> <cite> </cite> </blockquote> <h2 class=section-title id=opening-night><a href='#opening-night' class=section-inner>Opening Night</a></h2> <p>AHD culminated in an art show taking over the ground floor and mezzanine of PNCA. Every group installed its project and set it up to be on display for several hours that night.</p> <p><div class="image-wrap image-primary"> <img src="/images/ahd/show1.jpg" alt="A photo taken from overhead showing groups of people clustered around various artworks displayed on small white tables." title="The AHD exhibition at PNCA." width=800px height=600px /> </div></p> <p>The diversity of skills brought into AHD meant that a huge variety of projects came out of it. There were Kinect and HoloLens projects; there was 16mm film and a dating app for animated monsters; there was a DDR dance pad that made emoji rain across a VR scene.</p> <p>Our project was surprisingly easy to set up and ran beautifully all evening. We ran an exported Unity app on a Mac Mini and projected it into a fake smartphone cut out of foamcore.</p> <p><figure role=img class="image-wrap image-primary"> <img src="/images/ahd/group.jpg" alt="A photo of three people, two men and a woman. They stand next to a projector, in front of a false wall onto which is projected a mimicked texting exchange." title="My group poses with our chatbots." width=800px height=600px /> <figcaption> <p>Live at PNCA! From left to right: Steve, Alec, and me.</p> </figcaption> </figure></p> <p>Visitors ebbed and flowed through the gallery. Since our project wasn&#39;t interactive, I had been afraid that people might not feel interested or engaged, but we ended up with a piece that still drew visitors in and held their attention. Part of the interest, I think, came from being able to see a very topical, polemic bot (like <code>/r/The_Donald</code>), or to see bots of opposing views (<code>/r/Feminism</code> and <code>/r/TheRedPill</code>, for example) matched against each other.</p> <p><figure role=img class="image-wrap image-primary"> <img src="/images/ahd/show2.jpg" alt="A photo taken from overhead shows groups of people looking at art. One group is clustered around a projector; one person points at the projection." title="Our art at the opening show." width=800px height=600px /> <figcaption> <p>people â¤ï¸ looking â¤ï¸ at â¤ï¸ our â¤ï¸ art</p> </figcaption> </figure></p> <p>I loved working on this project and am looking for ways to keep working with it. Maybe as a foray into machine learning?</p> <p>Thanks to Alec and Steve for being amazing teammates and setting us all up for success on a super fun, challenging project. I look forward to lots more art-hacking in the future.</p> <hr> <p><em>This was my first blog post that wasn&#39;t about making this blog! Stay tuned for a post about Markov chains.</em></p> </div> </article> </main> <footer> <nav id=footer-nav role=navigation aria-label="Navigate to Other Pages" class="footer-item noprint"> <ul id=footer-nav-links> <li class=footer-nav-link> <a href="/" class=footer-unit>Index</a> </li> <li class=footer-nav-link> <a href="/info" class=footer-unit>Info</a> </li> <li class=footer-nav-link> <a href="#" class=footer-unit>Top</a> </li> </ul> </nav> <div id=footer-colophon role=contentinfo aria-label="Made with love in 2020" class="footer-item footer-unit noprint"> &hearts; 2020 </div> <div class="printonly aux"> http://violet.is || Violet Pe&ntilde;a || 2020 </div> </footer> </body> </html>